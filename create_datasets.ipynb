{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a18e1e9-c125-45b7-8a75-091941dc5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0ffb8a9-e9d1-412c-9b3b-b3ec44c0391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = pd.read_csv(\"archive/artists-data.csv\")\n",
    "lyrics = pd.read_csv(\"archive/lyrics-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "514dc0d4-daec-411d-96f2-960c7004ee87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44                        /beyonce/\n",
       "505                           /sia/\n",
       "334                        /anitta/\n",
       "2734                        /adele/\n",
       "1854                       /eminem/\n",
       "623                    /ed-sheeran/\n",
       "1017                 /harpa-crista/\n",
       "2023                     /coldplay/\n",
       "2735                         /pink/\n",
       "250                  /taylor-swift/\n",
       "2024              /imagine-dragons/\n",
       "335                 /justin-bieber/\n",
       "869                      /ludmilla/\n",
       "3132                  /the-beatles/\n",
       "2736                     /maroon-5/\n",
       "45                     /bruno-mars/\n",
       "3600    /zeze-di-camargo-e-luciano/\n",
       "336                     /lady-gaga/\n",
       "2025                 /lana-del-rey/\n",
       "337                 /ariana-grande/\n",
       "2739              /christina-perri/\n",
       "2986                 /phil-collins/\n",
       "46                        /rihanna/\n",
       "1018                  /cancao-nova/\n",
       "2413               /roberto-carlos/\n",
       "2760               /camila-cabello/\n",
       "1582                     /bon-jovi/\n",
       "2737                   /elton-john/\n",
       "48                    /john-legend/\n",
       "3442                  /john-lennon/\n",
       "49                  /racionais-mcs/\n",
       "226                    /pink-floyd/\n",
       "1583                    /scorpions/\n",
       "3139        /red-hot-chili-peppers/\n",
       "1019            /musicas-catolicas/\n",
       "47                        /50-cent/\n",
       "1564                      /nirvana/\n",
       "1585                        /queen/\n",
       "339                    /katy-perry/\n",
       "340                          /alok/\n",
       "2988                           /u2/\n",
       "52                /black-eyed-peas/\n",
       "50                /michael-jackson/\n",
       "2987                   /jason-mraz/\n",
       "1584                 /guns-n-roses/\n",
       "51                    /alicia-keys/\n",
       "1728                    /rammstein/\n",
       "2741                 /shawn-mendes/\n",
       "1586                  /linkin-park/\n",
       "338                       /shakira/\n",
       "Name: Link, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_artists = artists.sort_values(by=\"Popularity\", ascending=False).head(50)\n",
    "top_artists[\"Link\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6cfc25ff-55a6-4611-b967-0683f7abade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top_artists[\"Link\"]:\n",
    "    lyrics_artist = lyrics.query(f\"ALink == @i and language == 'en'\")\n",
    "    with open(f\"text_lyrics/{i[1:-1]}.txt\", \"w+\") as f:\n",
    "        f.write(\"/n\".join(lyrics_artist[\"Lyric\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4bb97687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "ds = None\n",
    "train_split = 0.8\n",
    "validation_split = 0.2\n",
    "with open(f\"text_lyrics/beyonce.txt\", \"r\") as f:\n",
    "    arr = f.readlines()\n",
    "    train, test, validation = np.split(\n",
    "        arr, [int(len(arr) * train_split), int(len(arr) * validation_split)]\n",
    "    )\n",
    "    train = np.delete(train, np.where(train == '\\n'))\n",
    "    validation = np.delete(validation, np.where(validation == '\\n'))\n",
    "    # ds = DatasetDict(\n",
    "    #     {\n",
    "    #         \"train\": Dataset.from_dict({\"text\": list(train)}),\n",
    "    #         \"validation\": Dataset.from_dict({\"text\": list(validation)})\n",
    "    #     }\n",
    "    # )\n",
    "    # ds.push_to_hub(\"BhavyaMuni/artist-lyrics\")\n",
    "    train_df = pd.DataFrame({\"text\": train})\n",
    "    train_df.to_csv(\"artist-lyrics/beyonce_data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "87d885fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /Users/bhavya/projects/hugging-face-playground/BhavyaMuni/artist-lyrics/beyonce_data/beyonce_data.py or any data file in the same directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_files \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtrain.csv\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m new_ds \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mload_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mBhavyaMuni/artist-lyrics/beyonce_data\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_files\u001b[39m=\u001b[39;49mdata_files)\n",
      "File \u001b[0;32m~/projects/hugging-face-playground/.env/lib/python3.10/site-packages/datasets/load.py:1734\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1731\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[1;32m   1733\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1734\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1735\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1736\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1737\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1738\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1739\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1740\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1741\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1742\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1743\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1744\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1745\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1746\u001b[0m )\n\u001b[1;32m   1748\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/projects/hugging-face-playground/.env/lib/python3.10/site-packages/datasets/load.py:1492\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1490\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1491\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1492\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1493\u001b[0m     path,\n\u001b[1;32m   1494\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1495\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1496\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1497\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1498\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1499\u001b[0m )\n\u001b[1;32m   1501\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/projects/hugging-face-playground/.env/lib/python3.10/site-packages/datasets/load.py:1218\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1220\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /Users/bhavya/projects/hugging-face-playground/BhavyaMuni/artist-lyrics/beyonce_data/beyonce_data.py or any data file in the same directory."
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "data_files = {\"train\": \"train.csv\"}\n",
    "new_ds = datasets.load_dataset(\"BhavyaMuni/artist-lyrics/beyonce_data\", data_files=data_files, split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447c4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "78bf3dfab2409d0de93bc88a070d69a2c578003fb5569f9b0436b24fbd75556d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
